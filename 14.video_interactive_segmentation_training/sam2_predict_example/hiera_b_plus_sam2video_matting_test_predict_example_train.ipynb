{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13316e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe29cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "inference_ipynb_path='/root/code/SimpleAICV_pytorch_training_examples/14.video_interactive_segmentation_training/sam2_predict_example'\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(inference_ipynb_path))\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from SimpleAICV.video_interactive_segmentation.models.segment_anything2_matting.sam2videomatting_test import hiera_b_plus_sam2video_matting_test\n",
    "from SimpleAICV.video_interactive_segmentation.common import load_state_dict\n",
    "\n",
    "sam2_checkpoint = ''\n",
    "\n",
    "sam2_model = hiera_b_plus_sam2video_matting_test()\n",
    "sam2_model = sam2_model.cuda()\n",
    "sam2_model = sam2_model.eval()\n",
    "\n",
    "load_state_dict(sam2_checkpoint, sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc36a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir_path = '/root/code/SimpleAICV_pytorch_training_examples/14.video_interactive_segmentation_training/sam2_predict_example/test_videos/bedroom'\n",
    "\n",
    "frames_name_list = []\n",
    "for per_frame_name in os.listdir(video_dir_path):\n",
    "    if '.jpg' in per_frame_name:\n",
    "        frames_name_list.append(per_frame_name)\n",
    "frames_name_list = sorted(frames_name_list)\n",
    "frames_path_list = [\n",
    "    os.path.join(video_dir_path, n) for n in frames_name_list\n",
    "]\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame: {frame_idx}\")\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    plt.imshow(show_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.init_video_state_dict(video_dir_path=video_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb95127",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.clear_video_state_dict_all_info(video_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.init_video_state_dict(video_dir_path=video_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started, for labels, `1` means positive click and `0` means negative click\n",
    "input_point = np.array([[210, 350]], dtype=np.float32)\n",
    "input_label = np.array([[1]], dtype=np.int32)\n",
    "print(input_point.shape, input_label.shape)\n",
    "\n",
    "input_prompt_point = np.concatenate([input_point, input_label], axis=1, dtype=np.float32)\n",
    "input_prompt_point = np.expand_dims(input_prompt_point, axis=0)\n",
    "print(input_prompt_point.shape,input_prompt_point.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=input_prompt_point,\n",
    "                                    prompt_box=None,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame: {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_points(input_point, input_label[0], plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b366e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_id = 0\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=0, tracking_object_ids=[tracking_object_id], use_point_prompt_input=True,use_box_prompt_input=False, use_mask_prompt_input=False)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state = video_state_dict['object_track_state'][tracking_object_id]\n",
    "object_track_result = video_state_dict['object_track_result'][tracking_object_id]\n",
    "print(frame_num, object_track_state)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1655bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started, for labels, `1` means positive click and `0` means negative click\n",
    "input_point = np.array([[210, 350], [250, 220]], dtype=np.float32)\n",
    "input_label = np.array([[1], [1]], dtype=np.int32)\n",
    "print(input_point.shape, input_label.shape)\n",
    "\n",
    "input_prompt_point = np.concatenate([input_point, input_label], axis=1, dtype=np.float32)\n",
    "input_prompt_point = np.expand_dims(input_prompt_point, axis=0)\n",
    "print(input_prompt_point.shape,input_prompt_point.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=input_prompt_point,\n",
    "                                    prompt_box=None,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_points(input_point, np.squeeze(input_label,axis=1), plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_id = 1\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=0, tracking_object_ids=[tracking_object_id], use_point_prompt_input=True,use_box_prompt_input=False, use_mask_prompt_input=False)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state = video_state_dict['object_track_state'][tracking_object_id]\n",
    "object_track_result = video_state_dict['object_track_result'][tracking_object_id]\n",
    "print(frame_num, object_track_state)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd06c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.clear_video_state_dict_all_object_info(video_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "input_box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
    "print(input_box.shape)\n",
    "\n",
    "input_prompt_box = np.expand_dims(input_box, axis=0)\n",
    "print(input_prompt_box.shape,input_prompt_box.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=None,\n",
    "                                    prompt_box=input_prompt_box,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_box(input_box, plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_id = 0\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=0, tracking_object_ids=[tracking_object_id], use_point_prompt_input=False,use_box_prompt_input=True, use_mask_prompt_input=False)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state = video_state_dict['object_track_state'][tracking_object_id]\n",
    "object_track_result = video_state_dict['object_track_result'][tracking_object_id]\n",
    "print(frame_num, object_track_state)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592329ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.clear_video_state_dict_all_object_info(video_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 60\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "input_box = np.array([180, 0, 450, 430], dtype=np.float32)\n",
    "print(input_box.shape)\n",
    "\n",
    "input_prompt_box = np.expand_dims(input_box, axis=0)\n",
    "print(input_prompt_box.shape,input_prompt_box.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=None,\n",
    "                                    prompt_box=input_prompt_box,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_box(input_box, plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_id = 0\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=60, tracking_object_ids=[tracking_object_id], use_point_prompt_input=False,use_box_prompt_input=True, use_mask_prompt_input=False)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state = video_state_dict['object_track_state'][tracking_object_id]\n",
    "object_track_result = video_state_dict['object_track_result'][tracking_object_id]\n",
    "print(frame_num, object_track_state)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.clear_video_state_dict_all_object_info(video_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 199\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "input_box = np.array([300, 50, 470, 375], dtype=np.float32)\n",
    "print(input_box.shape)\n",
    "\n",
    "input_prompt_box = np.expand_dims(input_box, axis=0)\n",
    "print(input_prompt_box.shape,input_prompt_box.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=None,\n",
    "                                    prompt_box=input_prompt_box,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_box(input_box, plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_id = 0\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=199, tracking_object_ids=[tracking_object_id], use_point_prompt_input=False,use_box_prompt_input=True, use_mask_prompt_input=False)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state = video_state_dict['object_track_state'][tracking_object_id]\n",
    "object_track_result = video_state_dict['object_track_result'][tracking_object_id]\n",
    "print(frame_num, object_track_state)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.clear_video_state_dict_all_object_info(video_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9383b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "prompt_mask_path= '/root/code/SimpleAICV_pytorch_training_examples/14.video_interactive_segmentation_training/sam2_predict_example/test_videos/bedroom/00000_prompt_mask_for_target_0.png'\n",
    "\n",
    "input_mask = np.array(Image.open(prompt_mask_path).convert('L'), dtype=np.uint8)\n",
    "input_mask = input_mask / 255.\n",
    "input_mask = input_mask.astype(np.float32)\n",
    "print(input_mask.shape)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_mask(input_mask, plt.gca())\n",
    "\n",
    "input_prompt_mask = input_mask\n",
    "print(input_prompt_mask.shape,input_prompt_mask.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=None,\n",
    "                                    prompt_box=None,\n",
    "                                    prompt_mask=input_prompt_mask)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f655139",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_id = 0\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=0, tracking_object_ids=[tracking_object_id], use_point_prompt_input=False,use_box_prompt_input=False, use_mask_prompt_input=True)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state = video_state_dict['object_track_state'][tracking_object_id]\n",
    "object_track_result = video_state_dict['object_track_result'][tracking_object_id]\n",
    "print(frame_num, object_track_state)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state_dict = sam2_model.clear_video_state_dict_all_object_info(video_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (150, 130, 290, 410) to get started\n",
    "input_box = np.array([150, 130, 290, 410], dtype=np.float32)\n",
    "print(input_box.shape)\n",
    "\n",
    "input_prompt_box = np.expand_dims(input_box, axis=0)\n",
    "print(input_prompt_box.shape,input_prompt_box.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=None,\n",
    "                                    prompt_box=input_prompt_box,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_box(input_box, plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "input_box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
    "print(input_box.shape)\n",
    "\n",
    "input_prompt_box = np.expand_dims(input_box, axis=0)\n",
    "print(input_prompt_box.shape,input_prompt_box.dtype)\n",
    "\n",
    "exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask = sam2_model.add_new_object_prompt_input(\n",
    "                                    video_state_dict,\n",
    "                                    frame_idx=frame_idx,\n",
    "                                    prompt_point=None,\n",
    "                                    prompt_box=input_prompt_box,\n",
    "                                    prompt_mask=None)\n",
    "print(exist_object_ids, frame_idx, new_object_id, has_prompt_point, has_prompt_box, has_prompt_mask)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "plt.imshow(show_image)\n",
    "show_box(input_box, plt.gca())\n",
    "\n",
    "global_preds, local_preds, fused_preds, iou_preds = sam2_model.forward_one_image_test(\n",
    "    video_state_dict, new_object_id, frame_idx, mask_out_idxs=[0, 1, 2, 3])\n",
    "global_preds = global_preds[0][0].permute(1, 2, 0).float().cpu().numpy()\n",
    "local_preds = local_preds[0][0][0].float().cpu().numpy()\n",
    "fused_preds = fused_preds[0][0][0].float().cpu().numpy()\n",
    "iou_preds = iou_preds[0][0].float().cpu().numpy()\n",
    "print(global_preds.shape, local_preds.shape, fused_preds.shape,\n",
    "      iou_preds.shape, iou_preds)\n",
    "\n",
    "local_preds = np.expand_dims(local_preds, axis=-1)\n",
    "fused_preds = np.expand_dims(fused_preds, axis=-1)\n",
    "print(local_preds.shape, np.max(local_preds), np.min(local_preds))\n",
    "print(fused_preds.shape, np.max(fused_preds), np.min(fused_preds))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(global_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, global pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(local_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, local pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(fused_preds)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, fused pred, IoU Score: {iou_preds:.3f}\",\n",
    "          fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 创建绿色背景\n",
    "show_image = Image.open(frames_path_list[frame_idx])\n",
    "green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "print(green_background.shape, np.max(green_background),\n",
    "      np.min(green_background))\n",
    "\n",
    "# 得到前景区域和背景区域并合并\n",
    "foreground = show_image * fused_preds\n",
    "background = green_background * (1 - fused_preds)\n",
    "result_image = foreground + background\n",
    "\n",
    "foreground = foreground.astype(np.uint8)\n",
    "background = background.astype(np.uint8)\n",
    "result_image = result_image.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(foreground)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(background)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(result_image)\n",
    "show_box(input_box, plt.gca())\n",
    "plt.title(f\"frame: {frame_idx}, IoU Score: {iou_preds:.3f}\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_object_ids = [0, 1]\n",
    "video_state_dict = sam2_model.forward_tracking_for_test(video_state_dict, start_tracking_frame_idx=0, tracking_object_ids=tracking_object_ids, use_point_prompt_input=False,use_box_prompt_input=True, use_mask_prompt_input=False)\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state_0 = video_state_dict['object_track_state'][0]\n",
    "object_track_result_0 = video_state_dict['object_track_result'][0]\n",
    "print(frame_num, object_track_state_0)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result_0[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "frame_num = video_state_dict['video_frame_num']\n",
    "object_track_state_1 = video_state_dict['object_track_state'][1]\n",
    "object_track_result_1 = video_state_dict['object_track_result'][1]\n",
    "print(frame_num, object_track_state_1)\n",
    "\n",
    "vis_frame_stride = 30\n",
    "for frame_idx in range(0, len(frames_path_list), vis_frame_stride):\n",
    "    per_frame_object_result = object_track_result_1[frame_idx]\n",
    "    per_frame_per_object_fuse_pred = per_frame_object_result['fuse_pred'][0]\n",
    "    per_frame_per_object_pred_iou = per_frame_object_result['pred_iou']\n",
    "    per_frame_per_pred_object_score = per_frame_object_result[\n",
    "        'pred_object_score']\n",
    "\n",
    "    show_image = Image.open(frames_path_list[frame_idx])\n",
    "    green_background = np.zeros_like(show_image, dtype=np.float32)\n",
    "    green_background[:, :] = [0, 255, 0]  # RGB格式\n",
    "\n",
    "    # 得到前景区域和背景区域并合并\n",
    "    per_frame_per_object_fuse_pred = np.expand_dims(\n",
    "        per_frame_per_object_fuse_pred, axis=-1)\n",
    "    foreground = show_image * per_frame_per_object_fuse_pred\n",
    "    background = green_background * (1 - per_frame_per_object_fuse_pred)\n",
    "    result_image = foreground + background\n",
    "    result_image = result_image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(\n",
    "        f\"frame: {frame_idx}, IoU Score: {per_frame_per_object_pred_iou:.3f}, Object Score: {per_frame_per_pred_object_score:.3f}\",\n",
    "        fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
